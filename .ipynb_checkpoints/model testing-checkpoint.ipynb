{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e913f1bf",
   "metadata": {},
   "source": [
    " <span style=\"color: purple; font-weight: bold; font-size:26px\"> Check if any models performs better than the naive classifier </span>\n",
    "\n",
    "\n",
    "Let's start by importing some libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# visualisations\n",
    "import seaborn \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "import pickle # for saving the model\n",
    "\n",
    "# preprocesing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# just out of curiosity\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52001656",
   "metadata": {},
   "source": [
    "Now, let's define some helper functions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9265a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions:\n",
    "\n",
    "def plot_models_acc(dict, naive_classifier):\n",
    "    labels = tuple(dict.keys())\n",
    "    y_pos = np.arange(len(labels))\n",
    "    values = [dict[n]['accuracy'] for n in dict]\n",
    "    plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, labels,rotation='vertical')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy of different models')\n",
    "    # add a horizontal line at naive_classfier\n",
    "    plt.axhline(y=naive_classifier, color='r', linestyle='-', label='Naive Classifier')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# checking the distribution of the features :)\n",
    "def feature_dist(dataframe):\n",
    "    col_num = len(dataframe.columns)\n",
    "    from_to_ind = [(i, i+6) for i in range(0, col_num, 6)]\n",
    "\n",
    "    for i, j in from_to_ind:\n",
    "        if j >= col_num:\n",
    "            if col_num - 1 == i:\n",
    "                dataframe.iloc[:, i].hist(figsize=(11,11))\n",
    "            else:\n",
    "                dataframe.iloc[:, i:col_num-1].hist(layout=(1,col_num - 1 - i), figsize=(11,11))\n",
    "        else:\n",
    "            dataframe.iloc[:, i:j].hist(layout=(2,3), figsize=(11,11))\n",
    "            \n",
    "            \n",
    "def conf_mat(grid_search: GridSearchCV, Y_test):\n",
    "    # construction confusion matrix\n",
    "    outcome_class_labels = ['Red', 'Draw', 'No contest', 'Blue']\n",
    "    cm = confusion_matrix(\n",
    "        Y_test, \n",
    "        grid_search.predict(X_test),\n",
    "        labels = outcome_class_labels\n",
    "    )\n",
    "\n",
    "    # create heatmap\n",
    "    seaborn.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        cmap='Blues', \n",
    "        xticklabels=outcome_class_labels,\n",
    "        yticklabels=outcome_class_labels,\n",
    "        fmt='d')\n",
    "\n",
    "    # add labels\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b51020",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "Let's import our data now. It will most likely change, since the older data seems to be a little weird. For example, most of the winners are on the red side, most of some data is missing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8385526",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./UFCdata/test.xlsx\")\n",
    "data = data.replace(['--', '---'], pd.NA)[3000:] # also just for now, but starting from 3000's row is a nice tradeoff\n",
    "# between a lower naive classifier accuracy and the completeness of the data\n",
    "data = data.dropna() # absolutely not final!!!!!!!! xD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc83d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d10e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc9aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb14d1",
   "metadata": {},
   "source": [
    "### Naive classifier\n",
    "\n",
    "The naive classifier that would be the least we should beat is the proportion of Red Winners to all fights outcomes, since it's the most common result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_classifier = data.loc[data['Winner'] == 'Red']['Winner'].count() / data['Winner'].count()\n",
    "print(\"The naive classifier accuracy is: \", naive_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffa9d3",
   "metadata": {},
   "source": [
    "It will be quite hard to beat, hence it explains the need to re-run the scraper to obtain more recent data about the fights, since the naive classifier will drop significantly to a little bit over 56%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598442b",
   "metadata": {},
   "source": [
    "We further need to drop some of the columns that will not be of use. I'll also remove closing odds since these will surely improve the model accuracy, but we shouldn't rely on them too much\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdata = data.drop(['Unnamed: 0',\n",
    "                      'Column1',\n",
    "                      'Event_Name',\n",
    "                      'Event_Location',\n",
    "                      'B_Name',\n",
    "                      'R_Name',\n",
    "                      'Conclusion_Method',\n",
    "                      'Event_Date',\n",
    "                      'Last_Round_Duration',\n",
    "                      'Number_of_Rounds',\n",
    "                      'Referee',\n",
    "                      'R_Closing_Odds', # bye bye closing odds...\n",
    "                      'B_Closing_Odds'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473eaaa2",
   "metadata": {},
   "source": [
    "Next, let's find numerical and categorical columns in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "objecttypes_cat = list(dropdata.select_dtypes(include=['O']).columns)\n",
    "objecttypes_num = list(dropdata.select_dtypes(include=['int64', 'float64']).columns)\n",
    "\n",
    "# for col in objecttypes:\n",
    "#     dropdata[col] = dropdata[col].astype('category')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ed5c3",
   "metadata": {},
   "source": [
    "## Data correlation\n",
    "\n",
    "Since we are givem a considerate number of features, instead of presenting all possible correlations, we'll stick to N most significant ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24024e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Correlation Matrix\n",
    "# corrmat = data.corr()\n",
    "# f, ax = plt.subplots(figsize=(12, 9))\n",
    "# seaborn.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15db33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Subset Correlation Matrix\n",
    "# k = 10 #number of variables for heatmap\n",
    "# corrmat = dropdata.corr()\n",
    "# cols = corrmat.nlargest(k, 'Winner')['Winner'].index\n",
    "# cm = np.corrcoef(dropdata[cols].values.T)\n",
    "# seaborn.set(font_scale=1.25)\n",
    "# hm = seaborn.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1231426",
   "metadata": {},
   "source": [
    "# Candidate models\n",
    "\n",
    "We've chosen the following candidate models that we will further try to get the best performance from. In the end, we will present a comparison of each model.\n",
    "\n",
    "#### - Perceptron\n",
    "#### - Random Forests\n",
    "#### - Decision Trees Classifier\n",
    "#### - SGD Classifier\n",
    "#### - Linear SVC\n",
    "#### - Gaussian Naive Bayes\n",
    "#### - KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902331f4",
   "metadata": {},
   "source": [
    "### Standardizing the data (based on X_train):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bce0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random.seed(2023)\n",
    "np.random.seed(2023)\n",
    "\n",
    "Y_all = dropdata['Winner']\n",
    "X_all = dropdata.drop(['Winner'], axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size=0.25, random_state=2023)\n",
    "\n",
    "# standardizer for numerical columns\n",
    "transformer_num = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# standardizer for categorical columns\n",
    "transformer_cat = Pipeline([\n",
    "    ('scaler', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# jumping into using these into the preprocessor for all features:\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', transformer_num, objecttypes_num),\n",
    "        ('cat', transformer_cat, objecttypes_cat[:-1])   # no 'Winner' encoded\n",
    "], remainder='passthrough')\n",
    "\n",
    "# the standardization should be based on the training set, hence we obtain the relevant parameters:\n",
    "preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a89b0",
   "metadata": {},
   "source": [
    "## Perceptron:\n",
    "\n",
    "The Perceptron is another simple classification algorithm suitable for large scale learning. By default:\n",
    "\n",
    "        It does not require a learning rate.\n",
    "\n",
    "        It is not regularized (penalized).\n",
    "\n",
    "        It updates its model only on mistakes.\n",
    "\n",
    "The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.\n",
    "\n",
    "- Linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe529a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', Perceptron(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [5, 10, 20, 30, 50, 70, 90],\n",
    "    'model__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'model__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'model__max_iter': [100, 150, 200, 250],\n",
    "    'model__tol': [1, 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2015954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('best_perceptron_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fecf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('best_perceptron_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270af42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing the initial, preprocessed columns:\n",
    "print('Columns in X_train: ', preprocessor.get_feature_names_out())\n",
    "\n",
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('\\nColumns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271356a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['Perceptron'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "tuned_models['Perceptron'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e6f4b",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "A random forest classifier.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd91b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', RandomForestClassifier(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [5, 10, 20, 30, 50, 70, 90],\n",
    "    'model__n_estimators': [50, 100, 200, 300],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_depth': [10, 30, 60, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=4)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('best_random_forest_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39135fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('best_random_forest_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56ff01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing the initial, preprocessed columns:\n",
    "print('Columns in X_train: ', preprocessor.get_feature_names_out())\n",
    "\n",
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('\\nColumns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ', acc_sco)\n",
    "tuned_models['Random_Forest'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a470cb",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "A decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac988d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', DecisionTreeClassifier(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [3, 5, 10, 20],\n",
    "    'model__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'model__max_depth': [2, 5, 10],\n",
    "    'model__min_samples_split': [2, 5, 10, 15, 20, 30],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 8, 10],\n",
    "    'model__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=3)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('best_decision_tree_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59407bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('best_decision_tree_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the initial, preprocessed columns:\n",
    "print('Columns in X_train: ', preprocessor.get_feature_names_out())\n",
    "\n",
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('\\nColumns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6158544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['Decision_Tree'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618658fb",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) Classifier\n",
    "\n",
    "A linear classifier with SGD training. Default implementation uses the loss='hinge' used for linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', SGDClassifier(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [5, 10, 20, 40, 60],\n",
    "    'model__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'model__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'model__max_iter': [100, 200, 300],\n",
    "    'model__tol': [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac519529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('best_SGDClassfier_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7007ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('best_SGDClassfier_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65087ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing the initial, preprocessed columns:\n",
    "print('Columns in X_train: ', preprocessor.get_feature_names_out())\n",
    "\n",
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('\\nColumns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db22164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['SGDClassfier'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c8a93",
   "metadata": {},
   "source": [
    "## Linear SVC\n",
    "\n",
    "Linear Support Vector Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad8ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', LinearSVC(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [40, 60, 70, 80],\n",
    "    'model__loss': ['hinge', 'squared_hinge'],\n",
    "    'model__C': [0.01, 0.1, 1, 10],\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__max_iter': [100, 200, 300],\n",
    "    'model__tol': [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ab1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('best_LinearSVC_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671309ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('best_LinearSVC_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd84ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the initial, preprocessed columns:\n",
    "print('Columns in X_train: ', preprocessor.get_feature_names_out())\n",
    "\n",
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('\\nColumns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['LinearSVC'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45dac8f",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes (GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', GaussianNB())\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [2, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('best_GaussianNB_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('best_GaussianNB_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a7b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing the initial, preprocessed columns:\n",
    "print('Columns in X_train: ', preprocessor.get_feature_names_out())\n",
    "\n",
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('\\nColumns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['Gaussian_NB'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437eed00",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "Classifier implementing the k-nearest neighbors vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [5, 10, 20, 40, 60],\n",
    "    'model__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'model__weights': ['uniform', 'distance'],\n",
    "    'model__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('best_KNN_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d444799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('best_KNN_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05280429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the initial, preprocessed columns:\n",
    "print('Columns in X_train: ', preprocessor.get_feature_names_out())\n",
    "\n",
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('\\nColumns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['KNN'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243ead1",
   "metadata": {},
   "source": [
    "# FINAL TUNEL MODELS COMPARISON\n",
    "\n",
    "Here we assess the performance of each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8221224",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models_acc(tuned_models, naive_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65385da5",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "If the odds are available before the fight, then there is nothing inherently wrong with using them as a predictor. However, it's important to keep in mind that any information used for prediction should be available at the time the prediction is made. If the model is trained on data that includes information (such as odds) that would not be available at the time of prediction, then the model may overfit to the training data and perform poorly on new, unseen data.\n",
    "\n",
    "Additionally, it's important to consider the ethics of using betting odds for predictive modeling. While it may be legal to use this information for research purposes, it could be seen as promoting gambling or taking advantage of vulnerable populations. It's important to approach this type of research with sensitivity and to consider the potential impacts of the research on society."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2fa33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
