{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e913f1bf",
   "metadata": {},
   "source": [
    " <span style=\"color: purple; font-weight: bold; font-size:26px\"> Check if any models performs better than the naive classifier </span>\n",
    "\n",
    "\n",
    "Let's start by importing some libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72e7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# visualisations\n",
    "import seaborn \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "import pickle # for saving the model\n",
    "\n",
    "# preprocesing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# just out of curiosity\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52001656",
   "metadata": {},
   "source": [
    "Now, let's define some helper functions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9265a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some helper functions:\n",
    "\n",
    "def plot_models_acc(dict, naive_classifier):\n",
    "    labels = tuple(dict.keys())\n",
    "    y_pos = np.arange(len(labels))\n",
    "    values = [dict[n]['accuracy'] for n in dict]\n",
    "    plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, labels,rotation='vertical')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy of different models')\n",
    "    # add a horizontal line at naive_classfier\n",
    "    plt.axhline(y=naive_classifier, color='r', linestyle='-', label='Naive Classifier')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# checking the distribution of the features :)\n",
    "def feature_dist(dataframe):\n",
    "    col_num = len(dataframe.columns)\n",
    "    from_to_ind = [(i, i+6) for i in range(0, col_num, 6)]\n",
    "\n",
    "    for i, j in from_to_ind:\n",
    "        if j >= col_num:\n",
    "            if col_num - 1 == i:\n",
    "                dataframe.iloc[:, i].hist(figsize=(11,11))\n",
    "            else:\n",
    "                dataframe.iloc[:, i:col_num-1].hist(layout=(1,col_num - 1 - i), figsize=(11,11))\n",
    "        else:\n",
    "            dataframe.iloc[:, i:j].hist(layout=(2,3), figsize=(11,11))\n",
    "            \n",
    "            \n",
    "def conf_mat(grid_search: GridSearchCV, Y_test):\n",
    "    # construction confusion matrix\n",
    "    outcome_class_labels = ['Red', 'Draw', 'No contest', 'Blue']\n",
    "    cm = confusion_matrix(\n",
    "        Y_test, \n",
    "        grid_search.predict(X_test),\n",
    "        labels = outcome_class_labels\n",
    "    )\n",
    "\n",
    "    # create heatmap\n",
    "    seaborn.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        cmap='Blues', \n",
    "        xticklabels=outcome_class_labels,\n",
    "        yticklabels=outcome_class_labels,\n",
    "        fmt='d')\n",
    "\n",
    "    # add labels\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b51020",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "Now, let's import the data.\n",
    "\n",
    "The data is a result of a web-scraping script written in Python that visits websites like ufcstats.com and bestfightodds.com in order to gather, store and compute fighting statistics and odds. Our initial time period ranges from 2023 (most recent fights) to as far as 1994, but we have decided to skip the oldest 2400 fights. \n",
    "\n",
    "There are several reasons on why we skipped it. First and foremost, most of the oldest data has every winner labeled as 'Red' what would lead to the biased results of our model. Secondly, a lot of data was missing, especially data about the odds. Finally, fighting formulas have changed over time, therefore we would rather not to base our models on the 'outdated' information, so to speak.\n",
    "\n",
    "Furthermore, let us mention that even in the selected time frame there are some missingness of the odds data that the scraper was unable to gather. Most of the times, it was because of the inconsistencies in the naming of fighters from the bestfightodds.com side. For example, one fighter could have 3 or 4 different pages simply because there were typos in names. To handle that, we manually mapped these odds into our datasets. In the main Excel file one may go and see the green-marked cells signifying the manual intervention. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8385526",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./UFCdata/datasets/UFC_fights_stats_complete.xlsx')\n",
    "data = data.sort_values('Event_Date', ascending=True).reset_index(drop=True)\n",
    "data = data.replace(['--', '---'], pd.NA)[2400:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc83d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>R_Height_cm</th>\n",
       "      <th>R_Weight_kg</th>\n",
       "      <th>R_Reach</th>\n",
       "      <th>R_Age</th>\n",
       "      <th>R_Total_Knockdowns</th>\n",
       "      <th>R_Total_Significant_Strikes_Attempted</th>\n",
       "      <th>R_Total_Significant_Strikes_Landed</th>\n",
       "      <th>R_Significant_Strikes_%_Landed</th>\n",
       "      <th>...</th>\n",
       "      <th>B_Opp_Significant_Strikes_in_Clinch_%_Landed</th>\n",
       "      <th>B_Opp_Average_Significant_Strikes_on_Ground_Attempted</th>\n",
       "      <th>B_Opp_Average_Significant_Strikes_on_Ground_Landed</th>\n",
       "      <th>B_Opp_Significant_Strikes_on_Ground_%_Landed</th>\n",
       "      <th>Number_of_Rounds</th>\n",
       "      <th>Last_Round_Duration</th>\n",
       "      <th>R_Open_Odds</th>\n",
       "      <th>B_Open_Odds</th>\n",
       "      <th>R_Closing_Odds</th>\n",
       "      <th>B_Closing_Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4624.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.00000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "      <td>4708.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3612.097281</td>\n",
       "      <td>3612.097281</td>\n",
       "      <td>177.486007</td>\n",
       "      <td>73.803455</td>\n",
       "      <td>182.092842</td>\n",
       "      <td>30.937128</td>\n",
       "      <td>1.967502</td>\n",
       "      <td>631.524851</td>\n",
       "      <td>279.340272</td>\n",
       "      <td>0.422360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522387</td>\n",
       "      <td>5.16545</td>\n",
       "      <td>3.596194</td>\n",
       "      <td>0.505321</td>\n",
       "      <td>2.429269</td>\n",
       "      <td>230.601742</td>\n",
       "      <td>-114.771453</td>\n",
       "      <td>37.722175</td>\n",
       "      <td>-137.179269</td>\n",
       "      <td>39.881903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2022.513871</td>\n",
       "      <td>2022.513871</td>\n",
       "      <td>9.244279</td>\n",
       "      <td>16.029015</td>\n",
       "      <td>11.308305</td>\n",
       "      <td>4.246129</td>\n",
       "      <td>2.727167</td>\n",
       "      <td>668.628788</td>\n",
       "      <td>291.850253</td>\n",
       "      <td>0.152530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320886</td>\n",
       "      <td>7.77080</td>\n",
       "      <td>5.448841</td>\n",
       "      <td>0.351317</td>\n",
       "      <td>1.006400</td>\n",
       "      <td>90.851972</td>\n",
       "      <td>223.974404</td>\n",
       "      <td>203.324964</td>\n",
       "      <td>302.604690</td>\n",
       "      <td>251.600065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>152.400000</td>\n",
       "      <td>52.154195</td>\n",
       "      <td>147.320000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-2300.000000</td>\n",
       "      <td>-900.000000</td>\n",
       "      <td>-3500.000000</td>\n",
       "      <td>-1500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1900.750000</td>\n",
       "      <td>1900.750000</td>\n",
       "      <td>170.180000</td>\n",
       "      <td>61.224490</td>\n",
       "      <td>175.260000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.382689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>-230.000000</td>\n",
       "      <td>-150.000000</td>\n",
       "      <td>-265.000000</td>\n",
       "      <td>-155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3580.500000</td>\n",
       "      <td>3580.500000</td>\n",
       "      <td>177.800000</td>\n",
       "      <td>70.294785</td>\n",
       "      <td>182.880000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>413.500000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>0.442389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>2.93750</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>-155.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>-152.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5392.250000</td>\n",
       "      <td>5392.250000</td>\n",
       "      <td>185.420000</td>\n",
       "      <td>83.900227</td>\n",
       "      <td>190.500000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>905.250000</td>\n",
       "      <td>402.250000</td>\n",
       "      <td>0.503885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745532</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7096.000000</td>\n",
       "      <td>7096.000000</td>\n",
       "      <td>210.820000</td>\n",
       "      <td>120.181406</td>\n",
       "      <td>213.360000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>6324.000000</td>\n",
       "      <td>2975.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>141.00000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>775.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1   Unnamed: 0  R_Height_cm  R_Weight_kg      R_Reach  \\\n",
       "count   4708.000000  4708.000000  4708.000000  4708.000000  4624.000000   \n",
       "mean    3612.097281  3612.097281   177.486007    73.803455   182.092842   \n",
       "std     2022.513871  2022.513871     9.244279    16.029015    11.308305   \n",
       "min       31.000000    31.000000   152.400000    52.154195   147.320000   \n",
       "25%     1900.750000  1900.750000   170.180000    61.224490   175.260000   \n",
       "50%     3580.500000  3580.500000   177.800000    70.294785   182.880000   \n",
       "75%     5392.250000  5392.250000   185.420000    83.900227   190.500000   \n",
       "max     7096.000000  7096.000000   210.820000   120.181406   213.360000   \n",
       "\n",
       "             R_Age  R_Total_Knockdowns  R_Total_Significant_Strikes_Attempted  \\\n",
       "count  4708.000000         4708.000000                            4708.000000   \n",
       "mean     30.937128            1.967502                             631.524851   \n",
       "std       4.246129            2.727167                             668.628788   \n",
       "min      18.000000            0.000000                               0.000000   \n",
       "25%      28.000000            0.000000                             146.000000   \n",
       "50%      31.000000            1.000000                             413.500000   \n",
       "75%      34.000000            3.000000                             905.250000   \n",
       "max      46.000000           20.000000                            6324.000000   \n",
       "\n",
       "       R_Total_Significant_Strikes_Landed  R_Significant_Strikes_%_Landed  \\\n",
       "count                         4708.000000                     4708.000000   \n",
       "mean                           279.340272                        0.422360   \n",
       "std                            291.850253                        0.152530   \n",
       "min                              0.000000                        0.000000   \n",
       "25%                             67.000000                        0.382689   \n",
       "50%                            184.000000                        0.442389   \n",
       "75%                            402.250000                        0.503885   \n",
       "max                           2975.000000                        1.000000   \n",
       "\n",
       "       ...  B_Opp_Significant_Strikes_in_Clinch_%_Landed  \\\n",
       "count  ...                                   4708.000000   \n",
       "mean   ...                                      0.522387   \n",
       "std    ...                                      0.320886   \n",
       "min    ...                                      0.000000   \n",
       "25%    ...                                      0.333333   \n",
       "50%    ...                                      0.633333   \n",
       "75%    ...                                      0.745532   \n",
       "max    ...                                      1.000000   \n",
       "\n",
       "       B_Opp_Average_Significant_Strikes_on_Ground_Attempted  \\\n",
       "count                                         4708.00000       \n",
       "mean                                             5.16545       \n",
       "std                                              7.77080       \n",
       "min                                              0.00000       \n",
       "25%                                              0.00000       \n",
       "50%                                              2.93750       \n",
       "75%                                              7.00000       \n",
       "max                                            141.00000       \n",
       "\n",
       "       B_Opp_Average_Significant_Strikes_on_Ground_Landed  \\\n",
       "count                                        4708.000000    \n",
       "mean                                            3.596194    \n",
       "std                                             5.448841    \n",
       "min                                             0.000000    \n",
       "25%                                             0.000000    \n",
       "50%                                             2.000000    \n",
       "75%                                             4.800000    \n",
       "max                                            84.000000    \n",
       "\n",
       "       B_Opp_Significant_Strikes_on_Ground_%_Landed  Number_of_Rounds  \\\n",
       "count                                   4708.000000       4708.000000   \n",
       "mean                                       0.505321          2.429269   \n",
       "std                                        0.351317          1.006400   \n",
       "min                                        0.000000          1.000000   \n",
       "25%                                        0.000000          1.000000   \n",
       "50%                                        0.629630          3.000000   \n",
       "75%                                        0.765019          3.000000   \n",
       "max                                        1.000000          5.000000   \n",
       "\n",
       "       Last_Round_Duration  R_Open_Odds  B_Open_Odds  R_Closing_Odds  \\\n",
       "count          4708.000000  4708.000000  4708.000000     4708.000000   \n",
       "mean            230.601742  -114.771453    37.722175     -137.179269   \n",
       "std              90.851972   223.974404   203.324964      302.604690   \n",
       "min               5.000000 -2300.000000  -900.000000    -3500.000000   \n",
       "25%             158.000000  -230.000000  -150.000000     -265.000000   \n",
       "50%             300.000000  -155.000000   125.000000     -152.000000   \n",
       "75%             300.000000   120.000000   175.000000      122.000000   \n",
       "max             300.000000   600.000000  1100.000000      775.000000   \n",
       "\n",
       "       B_Closing_Odds  \n",
       "count     4708.000000  \n",
       "mean        39.881903  \n",
       "std        251.600065  \n",
       "min      -1500.000000  \n",
       "25%       -155.000000  \n",
       "50%        120.000000  \n",
       "75%        200.000000  \n",
       "max       1000.000000  \n",
       "\n",
       "[8 rows x 210 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d10e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_Name</th>\n",
       "      <th>Event_Location</th>\n",
       "      <th>Fight_Weight</th>\n",
       "      <th>Fight_Gender</th>\n",
       "      <th>R_Name</th>\n",
       "      <th>R_Stance</th>\n",
       "      <th>B_Name</th>\n",
       "      <th>B_Stance</th>\n",
       "      <th>Time_Format</th>\n",
       "      <th>Referee</th>\n",
       "      <th>Conclusion_Method</th>\n",
       "      <th>Winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4708</td>\n",
       "      <td>4708</td>\n",
       "      <td>4708</td>\n",
       "      <td>4708</td>\n",
       "      <td>4708</td>\n",
       "      <td>4692</td>\n",
       "      <td>4708</td>\n",
       "      <td>4677</td>\n",
       "      <td>4708</td>\n",
       "      <td>4681</td>\n",
       "      <td>4708</td>\n",
       "      <td>4708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>401</td>\n",
       "      <td>137</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1303</td>\n",
       "      <td>3</td>\n",
       "      <td>1471</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>164</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>UFC 286: Edwards vs. Usman 3</td>\n",
       "      <td>Las Vegas, Nevada, USA</td>\n",
       "      <td>Lightweight</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jim Miller</td>\n",
       "      <td>Orthodox</td>\n",
       "      <td>Angela Hill</td>\n",
       "      <td>Orthodox</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Herb Dean</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>1526</td>\n",
       "      <td>745</td>\n",
       "      <td>4056</td>\n",
       "      <td>20</td>\n",
       "      <td>3526</td>\n",
       "      <td>16</td>\n",
       "      <td>3421</td>\n",
       "      <td>4226</td>\n",
       "      <td>678</td>\n",
       "      <td>1755</td>\n",
       "      <td>2673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Event_Name          Event_Location Fight_Weight  \\\n",
       "count                           4708                    4708         4708   \n",
       "unique                           401                     137           10   \n",
       "top     UFC 286: Edwards vs. Usman 3  Las Vegas, Nevada, USA  Lightweight   \n",
       "freq                              15                    1526          745   \n",
       "\n",
       "       Fight_Gender      R_Name  R_Stance       B_Name  B_Stance  \\\n",
       "count          4708        4708      4692         4708      4677   \n",
       "unique            2        1303         3         1471         3   \n",
       "top            Male  Jim Miller  Orthodox  Angela Hill  Orthodox   \n",
       "freq           4056          20      3526           16      3421   \n",
       "\n",
       "          Time_Format    Referee     Conclusion_Method Winner  \n",
       "count            4708       4681                  4708   4708  \n",
       "unique              3        164                    10      4  \n",
       "top     3 Rnd (5-5-5)  Herb Dean  Decision - Unanimous    Red  \n",
       "freq             4226        678                  1755   2673  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc9aa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff793a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the data before we proceed:\n",
    "data = data.sample(frac=1, random_state=2023).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598442b",
   "metadata": {},
   "source": [
    "We further need to drop some of the columns that will not be of use. I'll also remove closing odds since these will surely improve the model accuracy, but we shouldn't rely on them too much\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277d58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdata = data.drop(['Unnamed: 0',\n",
    "                      'Unnamed: 0.1',\n",
    "                      'Event_Name',\n",
    "                      'Event_Location',\n",
    "                      'B_Name',\n",
    "                      'R_Name',\n",
    "                      'Conclusion_Method',\n",
    "                      'Event_Date',\n",
    "                      'Last_Round_Duration',\n",
    "                      'Number_of_Rounds',\n",
    "                      'Referee',\n",
    "                      'R_Closing_Odds', # bye bye closing odds...\n",
    "                      'B_Closing_Odds'], axis=1)\n",
    "\n",
    "# droping rows with 'Open Stance' since it's so seldom (but shouldn't be present for 2400 row onward)\n",
    "dropdata = dropdata.drop(dropdata[dropdata['R_Stance'] == 'Open Stance'].index)\n",
    "dropdata = dropdata.drop(dropdata[dropdata['B_Stance'] == 'Open Stance'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473eaaa2",
   "metadata": {},
   "source": [
    "Next, let's find numerical and categorical columns in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6704eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "objecttypes_cat = list(dropdata.select_dtypes(include=['O']).columns)\n",
    "objecttypes_num = list(dropdata.select_dtypes(include=['int64', 'float64']).columns)\n",
    "\n",
    "# we don't want to one-hot encode 'Winner' since it's not really required\n",
    "# and the pipeline breaks if we do (it'll be looking for this var in \n",
    "# X_train&X_test but it's obviously not there)\n",
    "objecttypes_cat = [x for x in objecttypes_cat if x != 'Winner']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ed5c3",
   "metadata": {},
   "source": [
    "## Data correlation\n",
    "\n",
    "Since we are givem a considerate number of features, instead of presenting all possible correlations, we'll stick to N most significant ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15db33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Subset Correlation Matrix\n",
    "# k = 10 #number of variables for heatmap\n",
    "# corrmat = dropdata.corr()\n",
    "# cols = corrmat.nlargest(k, 'Winner')['Winner'].index\n",
    "# cm = np.corrcoef(dropdata[cols].values.T)\n",
    "# seaborn.set(font_scale=1.25)\n",
    "# hm = seaborn.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1231426",
   "metadata": {},
   "source": [
    "# Candidate models\n",
    "\n",
    "We've chosen the following candidate models that we will further try to get the best performance from. In the end, we will present a comparison of each model.\n",
    "\n",
    "#### - Perceptron\n",
    "#### - Random Forests\n",
    "#### - Decision Trees Classifier\n",
    "#### - SGD Classifier\n",
    "#### - Linear SVC\n",
    "#### - Gaussian Naive Bayes\n",
    "#### - KNN\n",
    "\n",
    "PS.\n",
    "Models can be loaded after they are saved. However, pay attention when loading them up after the kernel restart - loaded model may make predictions based on different training set. This means that it was partially trained on the current test set. It is due to the shuffling we make during train_test splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "830d651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0187d88",
   "metadata": {},
   "source": [
    "### Standardizing the data (based on X_train) and outputting the naive classifier for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14bce0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random.seed(2137)\n",
    "np.random.seed(2137)\n",
    "\n",
    "Y_all = dropdata['Winner']\n",
    "X_all = dropdata.drop(['Winner'], axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size=0.25, random_state=2023)\n",
    "\n",
    "# standardizer for numerical columns\n",
    "transformer_num = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# standardizer for categorical columns\n",
    "transformer_cat = Pipeline([\n",
    "    ('scaler', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# jumping into using these into the preprocessor for all features:\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', transformer_num, objecttypes_num),\n",
    "        ('cat', transformer_cat, objecttypes_cat)   # no 'Winner' encoded\n",
    "], remainder='passthrough')\n",
    "\n",
    "# the standardization should be based on the training set, hence we obtain the relevant parameters:\n",
    "x = preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b66d89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The naive classifier accuracy in the test set is:  0.5548003398470688\n"
     ]
    }
   ],
   "source": [
    "# printing the naive classifier on the test set\n",
    "naive_classifier = Y_test.loc[Y_test == 'Red'].count() / Y_test.count()\n",
    "print(\"The naive classifier accuracy in the test set is: \", naive_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dac35f",
   "metadata": {},
   "source": [
    "The naive classifier that would be the least we should beat is the proportion of Red Winners to all fights outcomes, since it's the most common result. \n",
    "\n",
    "It appears that our models should be capable to achieve at least 55.5% accuracy to beat the naive classifier. It signifies that we do better than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a89b0",
   "metadata": {},
   "source": [
    "## Perceptron:\n",
    "\n",
    "The Perceptron is another simple classification algorithm suitable for large scale learning. By default:\n",
    "\n",
    "        It does not require a learning rate.\n",
    "\n",
    "        It is not regularized (penalized).\n",
    "\n",
    "        It updates its model only on mistakes.\n",
    "\n",
    "The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.\n",
    "\n",
    "- Linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe529a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', Perceptron(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [10, 20, 30, 50, 90, 150, len(preprocessor.get_feature_names_out())],\n",
    "    'model__penalty': ['elasticnet', 'l2', 'l1', None],\n",
    "    'model__alpha': [0.01, 0.001, 0.0001, 0.00001],\n",
    "    'model__max_iter': [500, 1000, 1500],\n",
    "    'model__tol': [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=3, verbose=2)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2015954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('trained_models/best_perceptron_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fecf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('trained_models/best_perceptron_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270af42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('Columns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271356a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['Perceptron'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "tuned_models['Perceptron'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e6f4b",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "A random forest classifier.\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7dd91b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 800 candidates, totalling 4000 fits\n",
      "Accuracy on the test set:  0.6202209005947323\n",
      "\n",
      "Best parameters after the grid search:  {'model__criterion': 'gini', 'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 2, 'model__min_samples_split': 6, 'model__n_estimators': 100, 'selector__k': 60}\n",
      "\n",
      "Time taken for model to learn:  4636.163654088974  seconds\n"
     ]
    }
   ],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', RandomForestClassifier(random_state=123))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [5, 10, 15, 30, 45, 60, 90, 120, 150, len(preprocessor.get_feature_names_out())],\n",
    "    'model__n_estimators': [50, 100, 200, 300],\n",
    "    'model__criterion': ['gini'],\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_split': [2, 4, 6, 8],\n",
    "    'model__min_samples_leaf': [1, 2, 4, 8, 12],\n",
    "    'model__max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=4)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "030b0c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to run this cell? It will overwrite current best model. Y/N: Y\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('trained_models/best_random_forest_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39135fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters after the grid search:  {'model__criterion': 'gini', 'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 4, 'model__n_estimators': 200, 'selector__k': 90}\n"
     ]
    }
   ],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('trained_models/best_random_forest_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56ff01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('\\nColumns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ', acc_sco)\n",
    "tuned_models['Random_Forest'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a470cb",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "A decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac988d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('Imputer', KNNImputer()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', DecisionTreeClassifier(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid (k<5 check)\n",
    "param_grid = {\n",
    "    'selector__k': [3, 4, 5, 10, 15, 30, 50, 70, 90, len(preprocessor.get_feature_names_out())],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_depth': [None, 4],\n",
    "    'model__min_samples_split': [2],\n",
    "    'model__min_samples_leaf': [1],\n",
    "    'model__max_features': ['sqrt', None]\n",
    "}\n",
    "\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=4)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('trained_models/best_decision_tree_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59407bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('trained_models/best_decision_tree_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('Columns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6158544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['Decision_Tree'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618658fb",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) Classifier\n",
    "\n",
    "A linear classifier with SGD training. Default implementation uses the loss='hinge' used for linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', SGDClassifier(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [10, 20, 30, 40, 50, 70, 90, 150, len(preprocessor.get_feature_names_out())],\n",
    "    'model__alpha': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'model__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'model__max_iter': [200, 500, 1000],\n",
    "    'model__tol': [0.1, 0.01, 0.001, 0.0001]\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=3)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac519529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('trained_models/best_SGDClassfier_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7007ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('trained_models/best_SGDClassfier_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65087ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('Columns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db22164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['SGDClassfier'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c8a93",
   "metadata": {},
   "source": [
    "## Linear SVC\n",
    "\n",
    "Linear Support Vector Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad8ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', LinearSVC(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [5, 10, 20, 40, 60, 90, 130, len(preprocessor.get_feature_names_out())],\n",
    "    'model__loss': ['hinge'],\n",
    "    'model__C': [10, 1, 0.1, 0.01, 0.001],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__max_iter': [500, 1000, 1500],\n",
    "    'model__tol': [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=3)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ab1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('trained_models/best_LinearSVC_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671309ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('trained_models/best_LinearSVC_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd84ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('Columns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['LinearSVC'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45dac8f",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes (GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', GaussianNB())\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 120, 150, len(preprocessor.get_feature_names_out())],\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('trained_models/best_GaussianNB_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('trained_models/best_GaussianNB_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a7b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('Columns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['Gaussian_NB'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437eed00",
   "metadata": {},
   "source": [
    "## KNN\n",
    "\n",
    "Classifier implementing the k-nearest neighbors vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [5, 10, 20, 40, 60, 90, 140, len(preprocessor.get_feature_names_out())],\n",
    "    'model__n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'model__weights': ['uniform', 'distance'],\n",
    "    'model__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=3)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('trained_models/best_KNN_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d444799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('trained_models/best_KNN_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05280429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('Columns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['KNN'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa573a2b",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the timer to see how much time it takes\n",
    "start_time = time.time()\n",
    "\n",
    "# Let's define the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('selector', SelectKBest()),\n",
    "    ('model', LogisticRegression(random_state=2023))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'selector__k': [5, 10, 20, 40, 60, 90, 140, len(preprocessor.get_feature_names_out())],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__C': [0.01, 0.1, 1.0],\n",
    "    'model__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'model__max_iter': [50, 100, 150, 200, 300],\n",
    "    'model__tol': [0.001, 0.0001, 0.00001]\n",
    "}\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=3)\n",
    "\n",
    "# fit the grid search object to the data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "score = grid_search.score(X_test, Y_test)\n",
    "\n",
    "# stopping the timer and seeing the result: ------------------------\n",
    "end_time = time.time()\n",
    "\n",
    "print('Accuracy on the test set: ', score)\n",
    "print('\\nBest parameters after the grid search: ', grid_search.best_params_)\n",
    "print(\"\\nTime taken for model to learn: \", end_time - start_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk using pickle\n",
    "if input('Are you sure you want to run this cell? It will overwrite current best model. Y/N: ') == 'Y':\n",
    "    with open('trained_models/best_logistic_regression.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a13097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from disk using pickle\n",
    "with open('trained_models/best_logistic_regression.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    " \n",
    "print('Best parameters after the grid search: ', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected feature indices after SelectKBest() in the best estimator\n",
    "selected_feature_indices = model.best_estimator_.named_steps['selector'].get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = preprocessor.get_feature_names_out()[selected_feature_indices]\n",
    "print('Columns after feature selection: ', selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97adb1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing accuracy score:\n",
    "acc_sco = accuracy_score(Y_test, model.predict(X_test)) \n",
    "\n",
    "# construction confusion matrix\n",
    "cm = conf_mat(model, Y_test)\n",
    "\n",
    "# store model results in tuned_models dictionary\n",
    "\n",
    "print('Accuracy on the test set: ',acc_sco)\n",
    "tuned_models['Logistic_Regression'] = {'accuracy': acc_sco, 'model': model, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243ead1",
   "metadata": {},
   "source": [
    "# FINAL TUNEL MODELS COMPARISON\n",
    "\n",
    "Here we assess the performance of each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8221224",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models_acc(tuned_models, naive_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65385da5",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "If the odds are available before the fight, then there is nothing inherently wrong with using them as a predictor. However, it's important to keep in mind that any information used for prediction should be available at the time the prediction is made. If the model is trained on data that includes information (such as odds) that would not be available at the time of prediction, then the model may overfit to the training data and perform poorly on new, unseen data.\n",
    "\n",
    "Additionally, it's important to consider the ethics of using betting odds for predictive modeling. While it may be legal to use this information for research purposes, it could be seen as promoting gambling or taking advantage of vulnerable populations. It's important to approach this type of research with sensitivity and to consider the potential impacts of the research on society."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
